{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSpnWBP5ELSI"
      },
      "source": [
        "# 実践演習 Day 1：streamlitとFastAPIのデモ\n",
        "このノートブックでは以下の内容を学習します。\n",
        "\n",
        "- 必要なライブラリのインストールと環境設定\n",
        "- Hugging Faceからモデルを用いたStreamlitのデモアプリ\n",
        "- FastAPIとngrokを使用したAPIの公開方法\n",
        "\n",
        "演習を始める前に、HuggingFaceとngrokのアカウントを作成し、\n",
        "それぞれのAPIトークンを取得する必要があります。\n",
        "\n",
        "\n",
        "演習の時間では、以下の3つのディレクトリを順に説明します。\n",
        "\n",
        "1. 01_streamlit_UI\n",
        "2. 02_streamlit_app\n",
        "3. 03_FastAPI\n",
        "\n",
        "2つ目や3つ目からでも始められる様にノートブックを作成しています。\n",
        "\n",
        "復習の際にもこのノートブックを役立てていただければと思います。\n",
        "\n",
        "### 注意事項\n",
        "「02_streamlit_app」と「03_FastAPI」では、GPUを使用します。\n",
        "\n",
        "これらを実行する際は、Google Colab画面上のメニューから「編集」→ 「ノートブックの設定」\n",
        "\n",
        "「ハードウェアアクセラレーター」の項目の中から、「T4 GPU」を選択してください。\n",
        "\n",
        "このノートブックのデフォルトは「CPU」になっています。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhtHkJOgELSL"
      },
      "source": [
        "# 環境変数の設定（1~3共有）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-FjBp4MMQHM"
      },
      "source": [
        "GitHubから演習用のコードをCloneします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AIXMavdDEP8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cdc4ad-c04e-4be0-8b8d-e14ed7efb5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lecture-ai-engineering'...\n",
            "remote: Enumerating objects: 66, done.\u001b[K\n",
            "remote: Total 66 (delta 0), reused 0 (delta 0), pack-reused 66 (from 1)\u001b[K\n",
            "Receiving objects: 100% (66/66), 53.82 KiB | 6.73 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Sh-kawai/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC8n7yZ_vs1K"
      },
      "source": [
        "必要なAPIトークンを.envに設定します。\n",
        "\n",
        "「lecture-ai-engineering/day1」の配下に、「.env_template」ファイルが存在しています。\n",
        "\n",
        "隠しファイルのため表示されていない場合は、画面左側のある、目のアイコンの「隠しファイルの表示」ボタンを押してください。\n",
        "\n",
        "「.env_template」のファイル名を「.env」に変更します。「.env」ファイルを開くと、以下のような中身になっています。\n",
        "\n",
        "\n",
        "```\n",
        "HUGGINGFACE_TOKEN=\"hf-********\"\n",
        "NGROK_TOKEN=\"********\"\n",
        "```\n",
        "ダブルクオーテーションで囲まれた文字列をHuggingfaceのアクセストークンと、ngrokの認証トークンで書き変えてください。\n",
        "\n",
        "それぞれのアカウントが作成済みであれば、以下のURLからそれぞれのトークンを取得できます。\n",
        "\n",
        "- Huggingfaceのアクセストークン\n",
        "https://huggingface.co/docs/hub/security-tokens\n",
        "\n",
        "- ngrokの認証トークン\n",
        "https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "\n",
        "書き換えたら、「.env」ファイルをローカルのPCにダウンロードしてください。\n",
        "\n",
        "「01_streamlit_UI」から「02_streamlit_app」へ進む際に、CPUからGPUの利用に切り替えるため、セッションが一度切れてしまいます。\n",
        "\n",
        "その際に、トークンを設定した「.env」ファイルは再作成することになるので、その手間を減らすために「.env」ファイルをダウンロードしておくと良いです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py1BFS5RqcSS"
      },
      "source": [
        "「.env」ファイルを読み込み、環境変数として設定します。次のセルを実行し、最終的に「True」が表示されていればうまく読み込めています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bvEowFfg5lrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00e30e36-e9cb-488e-81f0-2857feac4596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "/content/lecture-ai-engineering/day1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install python-dotenv\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "%cd /content/lecture-ai-engineering/day1\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os0Yk6gaELSM"
      },
      "source": [
        "# 01_streamlit_UI\n",
        "\n",
        "ディレクトリ「01_streamlit_UI」に移動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S28XgOm0ELSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b05669-daa7-47dd-b0cb-40b0484d214b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/01_streamlit_UI\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/01_streamlit_UI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVp-aEIkELSM"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBe41LFiELSN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyw6VHaTELSN"
      },
      "source": [
        "ngrokのトークンを使用して、認証を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYw1q0iXELSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc28cf7-8377-4387-b546-8c85a4fcb159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RssTcD_IELSN"
      },
      "source": [
        "アプリを起動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-E7ucR6ELSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be910c5-d050-4152-9d7d-41e30c9fcafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "公開URL: https://595a-35-199-25-120.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.199.25.120:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"公開URL: {public_url}\")\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbYyXVFjELSN"
      },
      "source": [
        "公開URLの後に記載されているURLにブラウザでアクセスすると、streamlitのUIが表示されます。\n",
        "\n",
        "app.pyのコメントアウトされている箇所を編集することで、UIがどの様に変化するか確認してみましょう。\n",
        "\n",
        "streamlitの公式ページには、ギャラリーページがあります。\n",
        "\n",
        "streamlitを使うとpythonという一つの言語であっても、様々なUIを実現できることがわかると思います。\n",
        "\n",
        "https://streamlit.io/gallery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmtP5GLOELSN"
      },
      "source": [
        "後片付けとして、使う必要のないngrokのトンネルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ek9QgahELSO"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-T8tFpyELSO"
      },
      "source": [
        "# 02_streamlit_app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqogFQKnELSO"
      },
      "source": [
        "\n",
        "ディレクトリ「02_streamlit_app」に移動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeEjlJ7uELSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d376d47e-cf65-4a36-9070-6ccfc792b292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/02_streamlit_app\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/02_streamlit_app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XUH2AstELSO"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDqvI4V3ELSO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO31umGZELSO"
      },
      "source": [
        "ngrokとhuggigfaceのトークンを使用して、認証を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPxTiEWQELSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "737fa90f-f431-4cf4-de5d-2815a05174e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `AI_Engineering_2025` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `AI_Engineering_2025`\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz4WrELLELSP"
      },
      "source": [
        "stramlitでHuggingfaceのトークン情報を扱うために、streamlit用の設定ファイル（.streamlit）を作成し、トークンの情報を格納します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W184-a7qFP0W"
      },
      "outputs": [],
      "source": [
        "# .streamlit/secrets.toml ファイルを作成\n",
        "import os\n",
        "import toml\n",
        "\n",
        "# 設定ファイルのディレクトリ確保\n",
        "os.makedirs('.streamlit', exist_ok=True)\n",
        "\n",
        "# 環境変数から取得したトークンを設定ファイルに書き込む\n",
        "secrets = {\n",
        "    \"huggingface\": {\n",
        "        \"token\": os.environ.get(\"HUGGINGFACE_TOKEN\", \"\")\n",
        "    }\n",
        "}\n",
        "\n",
        "# 設定ファイルを書き込む\n",
        "with open('.streamlit/secrets.toml', 'w') as f:\n",
        "    toml.dump(secrets, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK0vI_xKELSP"
      },
      "source": [
        "アプリを起動します。\n",
        "\n",
        "02_streamlit_appでは、Huggingfaceからモデルをダウンロードするため、初回起動には2分程度時間がかかります。\n",
        "\n",
        "この待ち時間を利用して、app.pyのコードを確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBQyTTWTELSP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c15d39-ae41-4811-c94f-532fbb7fcaea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "公開URL: https://bc29-34-141-200-111.ngrok-free.app\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.141.200.111:8501\u001b[0m\n",
            "\u001b[0m\n",
            "NLTK loaded successfully.\n",
            "2025-04-22 09:01:40.426439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745312500.700524    7081 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745312500.777516    7081 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 09:01:41.346236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "config.json: 100% 805/805 [00:00<00:00, 4.63MB/s]\n",
            "model.safetensors.index.json: 100% 24.2k/24.2k [00:00<00:00, 809kB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/241M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.99G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.99G [00:00<00:26, 188MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 10.5M/241M [00:00<00:03, 68.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.99G [00:00<00:22, 220MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 31.5M/241M [00:00<00:01, 128MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.99G [00:00<00:22, 218MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 62.9M/241M [00:00<00:01, 152MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.99G [00:00<00:23, 208MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 83.9M/241M [00:00<00:01, 150MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 105M/241M [00:00<00:00, 161MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.99G [00:00<00:24, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.99G [00:00<00:25, 192MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 136M/241M [00:00<00:00, 181MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 157M/241M [00:00<00:00, 186MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.99G [00:00<00:24, 199MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 178M/241M [00:01<00:00, 180MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.99G [00:01<00:26, 177MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 199M/241M [00:01<00:00, 165MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.99G [00:01<00:26, 181MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 220M/241M [00:01<00:00, 146MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.99G [00:01<00:28, 163MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 241M/241M [00:01<00:00, 146MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.99G [00:01<00:29, 157MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.99G [00:01<00:25, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.99G [00:01<00:24, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.99G [00:02<00:21, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.99G [00:02<00:21, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.99G [00:02<00:18, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.99G [00:02<00:18, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.99G [00:02<00:17, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.99G [00:02<00:17, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.99G [00:04<01:40, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.99G [00:04<01:08, 63.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.99G [00:05<00:52, 81.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.99G [00:05<00:43, 99.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.99G [00:05<00:36, 116MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.99G [00:05<00:29, 141MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.99G [00:05<00:25, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.99G [00:05<00:20, 200MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 870M/4.99G [00:05<00:18, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.99G [00:05<00:17, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.99G [00:06<00:16, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.99G [00:06<00:15, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.99G [00:06<00:15, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.99G [00:06<00:14, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 1.07G/4.99G [00:06<00:13, 288MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.99G [00:06<00:12, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.99G [00:06<00:12, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.99G [00:06<00:12, 315MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.99G [00:07<00:14, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 1.27G/4.99G [00:07<00:13, 278MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.99G [00:07<00:13, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.99G [00:07<00:13, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.99G [00:07<00:12, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.99G [00:07<00:13, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.99G [00:07<00:14, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.99G [00:08<00:12, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.99G [00:08<00:13, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.99G [00:08<00:14, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.99G [00:08<00:13, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.99G [00:08<00:13, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.99G [00:08<00:12, 265MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.67G/4.99G [00:08<00:12, 265MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.99G [00:10<01:15, 43.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.99G [00:11<00:50, 63.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.99G [00:11<00:36, 88.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.99G [00:11<00:27, 115MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.99G [00:11<00:23, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.99G [00:11<00:21, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.92G/4.99G [00:11<00:18, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.99G [00:11<00:15, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.99G [00:11<00:14, 213MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.99G [00:12<00:12, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.99G [00:12<00:10, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.99G [00:12<00:11, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.12G/4.99G [00:12<00:11, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.99G [00:12<00:11, 256MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.99G [00:12<00:11, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.99G [00:12<00:11, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.99G [00:12<00:11, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.99G [00:13<00:10, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.32G/4.99G [00:13<00:09, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.99G [00:13<00:09, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.99G [00:13<00:08, 303MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.99G [00:13<00:08, 305MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  49% 2.46G/4.99G [00:13<00:08, 295MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.99G [00:13<00:08, 304MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.99G [00:13<00:08, 299MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.99G [00:14<00:08, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.61G/4.99G [00:14<00:08, 269MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.99G [00:14<00:08, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.99G [00:14<00:08, 277MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.72G/4.99G [00:14<00:09, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.99G [00:14<00:08, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.99G [00:14<00:10, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.99G [00:15<00:08, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.99G [00:15<00:09, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.99G [00:15<00:08, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.99G [00:15<00:07, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.99G [00:15<00:07, 269MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.99G [00:15<00:07, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.99G [00:15<00:07, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.99G [00:15<00:07, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.99G [00:16<00:07, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  62% 3.11G/4.99G [00:16<00:07, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.99G [00:16<00:07, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.99G [00:16<00:07, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.21G/4.99G [00:16<00:07, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.99G [00:16<00:07, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.99G [00:17<00:10, 166MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.99G [00:17<00:08, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.99G [00:17<00:07, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 3.37G/4.99G [00:17<00:07, 221MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.99G [00:17<00:06, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.99G [00:17<00:06, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.46G/4.99G [00:17<00:06, 245MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.99G [00:17<00:06, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.99G [00:18<00:06, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.99G [00:18<00:05, 246MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.99G [00:18<00:05, 249MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.99G [00:18<00:05, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.99G [00:18<00:05, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.99G [00:18<00:04, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.71G/4.99G [00:18<00:04, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.99G [00:18<00:04, 265MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.99G [00:18<00:04, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 3.81G/4.99G [00:19<00:04, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.99G [00:19<00:04, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.99G [00:19<00:03, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.99G [00:19<00:03, 284MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.99G [00:19<00:03, 290MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.96G/4.99G [00:19<00:03, 297MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 4.01G/4.99G [00:19<00:03, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.99G [00:19<00:03, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.99G [00:19<00:03, 297MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.99G [00:21<00:12, 72.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.99G [00:21<00:09, 89.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.16G/4.99G [00:22<00:16, 50.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.99G [00:23<00:22, 35.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.99G [00:23<00:15, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.99G [00:24<00:11, 65.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.99G [00:24<00:08, 84.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.99G [00:24<00:05, 117MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.36G/4.99G [00:24<00:04, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.40G/4.99G [00:24<00:03, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.99G [00:24<00:02, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.99G [00:24<00:02, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.99G [00:24<00:01, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.55G/4.99G [00:25<00:01, 260MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.99G [00:25<00:01, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.61G/4.99G [00:25<00:01, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.99G [00:25<00:01, 282MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.99G [00:25<00:01, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.71G/4.99G [00:25<00:01, 274MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.99G [00:25<00:00, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.99G [00:25<00:00, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  96% 4.80G/4.99G [00:26<00:00, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.99G [00:26<00:00, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.99G [00:26<00:00, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.99G [00:26<00:00, 241MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.99G [00:26<00:00, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 4.99G/4.99G [00:26<00:00, 186MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:26<00:00, 13.47s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 11.13it/s]\n",
            "generation_config.json: 100% 168/168 [00:00<00:00, 924kB/s]\n",
            "tokenizer_config.json: 100% 46.9k/46.9k [00:00<00:00, 60.1MB/s]\n",
            "tokenizer.model: 100% 4.24M/4.24M [00:00<00:00, 101MB/s]\n",
            "tokenizer.json: 100% 17.5M/17.5M [00:00<00:00, 301MB/s]\n",
            "special_tokens_map.json: 100% 555/555 [00:00<00:00, 3.10MB/s]\n",
            "Device set to use cuda\n",
            "2025-04-22 09:02:44.068 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "2025-04-22 09:04:09.394 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Generated response in 9.39s\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "Data saved to DB successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Generated response in 27.44s\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Data saved to DB successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "Generated response in 29.75s\n",
            "NLTK Punkt data checked/downloaded.\n",
            "Database 'chat_feedback.db' initialized successfully.\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in: <module 'threading' from '/usr/lib/python3.11/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1560, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/server.py\", line 470, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 337, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8501).public_url\n",
        "print(f\"公開URL: {public_url}\")\n",
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKl4ogdQqNBw"
      },
      "source": [
        "アプリケーションの機能としては、チャット機能や履歴閲覧があります。\n",
        "\n",
        "これらの機能を実現するためには、StreamlitによるUI部分だけではなく、SQLiteを使用したチャット履歴の保存やLLMのモデルを呼び出した推論などの処理を組み合わせることで実現しています。\n",
        "\n",
        "- **`app.py`**: アプリケーションのエントリーポイント。チャット機能、履歴閲覧、サンプルデータ管理のUIを提供します。\n",
        "- **`ui.py`**: チャットページや履歴閲覧ページなど、アプリケーションのUIロジックを管理します。\n",
        "- **`llm.py`**: LLMモデルのロードとテキスト生成を行うモジュール。\n",
        "- **`database.py`**: SQLiteデータベースを使用してチャット履歴やフィードバックを保存・管理します。\n",
        "- **`metrics.py`**: BLEUスコアやコサイン類似度など、回答の評価指標を計算するモジュール。\n",
        "- **`data.py`**: サンプルデータの作成やデータベースの初期化を行うモジュール。\n",
        "- **`config.py`**: アプリケーションの設定（モデル名やデータベースファイル名）を管理します。\n",
        "- **`requirements.txt`**: このアプリケーションを実行するために必要なPythonパッケージ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvm8sWFPELSP"
      },
      "source": [
        "後片付けとして、使う必要のないngrokのトンネルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJC2TmZELSP"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUXhIzV7ELSP"
      },
      "source": [
        "# 03_FastAPI\n",
        "\n",
        "ディレクトリ「03_FastAPI」に移動します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4ejjDLxr3kfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64238f0a-91e8-4caa-deb9-d70a6c5fbce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lecture-ai-engineering/day1/03_FastAPI\n"
          ]
        }
      ],
      "source": [
        "%cd /content/lecture-ai-engineering/day1/03_FastAPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45TDsNzELSQ"
      },
      "source": [
        "必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9uv6glCz5a7Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfrmE2VmELSQ"
      },
      "source": [
        "ngrokとhuggigfaceのトークンを使用して、認証を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ELzWhMFORRIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88508037-ec68-4f3c-9862-b0af5eadaa2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `AI_Engineering_2025` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `AI_Engineering_2025`\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken $$NGROK_TOKEN\n",
        "!huggingface-cli login --token $$HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wztc2CELSQ"
      },
      "source": [
        "アプリを起動します。\n",
        "\n",
        "「02_streamlit_app」から続けて「03_FastAPI」を実行している場合は、モデルのダウンロードが済んでいるため、すぐにサービスが立ち上がります。\n",
        "\n",
        "「03_FastAPI」のみを実行している場合は、初回の起動時にモデルのダウンロードが始まるので、モデルのダウンロードが終わるまで数分間待ちましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "meQ4SwISn3IQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d2a6dc-4a59-4015-fb93-43c1e4118ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-05 06:46:51.712451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746427611.741770    3476 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746427611.749977    3476 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-05 06:46:51.801703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "モデル名を設定: google/gemma-2-2b-jpn-it\n",
            "/content/lecture-ai-engineering/day1/03_FastAPI/app.py:134: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n",
            "FastAPIエンドポイントを定義しました。\n",
            "アクティブなngrokトンネルはありません。\n",
            "ポート8501に新しいngrokトンネルを開いています...\n",
            "---------------------------------------------------------------------\n",
            "✅ 公開URL:   https://d704-34-124-167-236.ngrok-free.app\n",
            "📖 APIドキュメント (Swagger UI): https://d704-34-124-167-236.ngrok-free.app/docs\n",
            "---------------------------------------------------------------------\n",
            "(APIクライアントやブラウザからアクセスするためにこのURLをコピーしてください)\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m3476\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "load_model_task: モデルの読み込みを開始...\n",
            "使用デバイス: cuda\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00, 13.33it/s]\n",
            "Device set to use cuda\n",
            "モデル 'google/gemma-2-2b-jpn-it' の読み込みに成功しました\n",
            "load_model_task: モデルの読み込みが完了しました。\n",
            "起動時にモデルの初期化が完了しました。\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8501\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     222.150.196.5:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     222.150.196.5:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     222.150.196.5:0 - \"\u001b[1mGET /docs HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     222.150.196.5:0 - \"\u001b[1mGET /openapi.json HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     222.150.196.5:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     34.41.23.227:0 - \"\u001b[1mGET /health HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     34.41.23.227:0 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     34.41.23.227:0 - \"\u001b[1mGET /health HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     34.41.23.227:0 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     34.41.23.227:0 - \"\u001b[1mGET /health HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "シンプルなリクエストを受信: prompt=AIについて100文字で教えてください..., max_new_tokens=512\n",
            "モデル推論を開始...\n",
            "モデル推論が完了しました。\n",
            "抽出されたアシスタント応答: 。\n",
            "\n",
            "「AI」は、人工知能の略称で、人間のような思考や判断能力を持つコンピュータを指します。\n",
            "学習や分析能力を有し、人間と協力して様々なタスクを実行することができます。...\n",
            "応答生成時間: 5.00秒\n",
            "\u001b[32mINFO\u001b[0m:     34.41.23.227:0 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "シンプルなリクエストを受信: prompt=What is Momotaro story?..., max_new_tokens=512\n",
            "モデル推論を開始...\n",
            "モデル推論が完了しました。\n",
            "抽出されたアシスタント応答: The Momotaro story is a Japanese folktale that has been passed down for generations. It tells the ta...\n",
            "応答生成時間: 27.89秒\n",
            "\u001b[32mINFO\u001b[0m:     44.200.191.101:0 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "シンプルなリクエストを受信: prompt=初めまして。あなたはどのAPIを使用して動作していますか？..., max_new_tokens=512\n",
            "モデル推論を開始...\n",
            "モデル推論が完了しました。\n",
            "抽出されたアシスタント応答: **回答:**\n",
            "\n",
            "こんにちは！私はGoogleによってトレーニングされた大規模言語モデルです。 私はGoogleによって開発されたAPIを使用して動作しています。...\n",
            "応答生成時間: 2.12秒\n",
            "\u001b[32mINFO\u001b[0m:     44.200.191.101:0 - \"\u001b[1mPOST /generate HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "t=2025-05-05T08:02:52+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-dd1afa0e-0d33-43c9-9a62-03b7a690357f acceptErr=\"failed to accept connection: Listener closed\"\n",
            "t=2025-05-05T08:02:52+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-dd1afa0e-0d33-43c9-9a62-03b7a690357f err=\"failed to start tunnel: session closed\"\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m3476\u001b[0m]\n",
            "\n",
            "サーバープロセスが終了しました。\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 699, in lifespan\n",
            "    await receive()\n",
            "GeneratorExit\n",
            "\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-2' coro=<LifespanOn.main() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/lifespan/on.py:78> wait_for=<Future cancelled>>\n"
          ]
        }
      ],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLubjIhbELSR"
      },
      "source": [
        "FastAPIが起動すると、APIとクライアントが通信するためのURL（エンドポイント）が作られます。\n",
        "\n",
        "URLが作られるのと合わせて、Swagger UIというWebインターフェースが作られます。\n",
        "\n",
        "Swagger UIにアクセスすることで、APIの仕様を確認できたり、APIをテストすることができます。\n",
        "\n",
        "Swagger UIを利用することで、APIを通してLLMを動かしてみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgumW3mGELSR"
      },
      "source": [
        "後片付けとして、使う必要のないngrokのトンネルを削除します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJymTZio-WPJ"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}